{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39384\n",
      "37550\n",
      "27058\n",
      "41541\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "'''\n",
    "102 娱乐 娱乐 news_entertainment\n",
    "103 体育 体育 news_sport\n",
    "108 教育 教育 news_edu \n",
    "109 科技 科技 news_tech\n",
    "'''\n",
    "\n",
    "\n",
    "def select_data(path='data/toutiao_cat_data.txt', size=2500):\n",
    "    story = []\n",
    "    sport = []\n",
    "    edu = []\n",
    "    tech = []\n",
    "    try:\n",
    "        with open(path) as f:    \n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                # print(line[24])\n",
    "                if line[23:25] == '02':\n",
    "                    story.append(line)\n",
    "                elif line[23:25] == '03':\n",
    "                    sport.append(line)\n",
    "                elif line[23:25] == '08':\n",
    "                    edu.append(line)\n",
    "                elif line[23:25] == '09':\n",
    "                    tech.append(line)\n",
    "                line = f.readline()\n",
    "    except:\n",
    "        print('error')\n",
    "    print(len(story))\n",
    "    print(len(sport))\n",
    "    print(len(edu))\n",
    "    print(len(tech))\n",
    "    story = random.sample(story, size)\n",
    "    sport = random.sample(sport, size)\n",
    "    edu = random.sample(edu, size)\n",
    "    tech = random.sample(tech, size)\n",
    "    with open(\"data/data.txt\", \"a\") as f:\n",
    "        for line in story:\n",
    "            f.write(line)\n",
    "        for line in sport:\n",
    "            f.write(line)\n",
    "        for line in edu:\n",
    "            f.write(line)\n",
    "        for line in tech:\n",
    "            f.write(line)\n",
    "\n",
    "select_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sympy import apart\n",
    "\n",
    "def pre_process(path='data/data.txt'):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    corpus = []\n",
    "    value = re.compile(r'^[\\u4e00-\\u9fa5]{2,}$')\n",
    "    try:\n",
    "        stopwords = open('data/stopwords.txt')\n",
    "        for word in stopwords:\n",
    "            word = word.strip()\n",
    "            texts.append(word)\n",
    "    except: \n",
    "        print('error')\n",
    "    \n",
    "    try:\n",
    "        with open(path) as f:    \n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                info = line.split('_!_')\n",
    "                label = info[1]\n",
    "                if label == '102':\n",
    "                    labels.append(0)\n",
    "                elif label == '103':\n",
    "                    labels.append(1)\n",
    "                elif label == '108':\n",
    "                    labels.append(2)\n",
    "                elif label == '109':\n",
    "                    labels.append(3)\n",
    "                data1 = jieba.cut(info[3])\n",
    "                data2 = info[4][:-1].split(',')\n",
    "                data_adj = ''\n",
    "                for item in data1:\n",
    "                    if item not in texts:\n",
    "                        if value.match(item):\n",
    "                            data_adj += item+' '\n",
    "                for item in data2:\n",
    "                    if item not in texts:\n",
    "                        if value.match(item):\n",
    "                            data_adj += item+' '\n",
    "                corpus.append(data_adj[:-1])\n",
    "                line = f.readline()\n",
    "    except:\n",
    "        print('error')\n",
    "    \n",
    "    with open('data/data1.txt', \"a\") as f:\n",
    "        for line in corpus:\n",
    "            f.write(line + '\\n')\n",
    "        print(len(corpus))\n",
    "\n",
    "pre_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取全部数据，用于word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39384\n",
      "37550\n",
      "27058\n",
      "41541\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import random\n",
    "\n",
    "'''\n",
    "102 娱乐 娱乐 news_entertainment\n",
    "103 体育 体育 news_sport\n",
    "108 教育 教育 news_edu \n",
    "109 科技 科技 news_tech\n",
    "'''\n",
    "\n",
    "\n",
    "def select_data(path='data/toutiao_cat_data.txt', size=1000):\n",
    "    story = []\n",
    "    sport = []\n",
    "    edu = []\n",
    "    tech = []\n",
    "    try:\n",
    "        with open(path) as f:    \n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                # print(line[24])\n",
    "                if line[23:25] == '02':\n",
    "                    story.append(line)\n",
    "                elif line[23:25] == '03':\n",
    "                    sport.append(line)\n",
    "                elif line[23:25] == '08':\n",
    "                    edu.append(line)\n",
    "                elif line[23:25] == '09':\n",
    "                    tech.append(line)\n",
    "                line = f.readline()\n",
    "    except:\n",
    "        print('error')\n",
    "    print(len(story))\n",
    "    print(len(sport))\n",
    "    print(len(edu))\n",
    "    print(len(tech))\n",
    "    # story = random.sample(story, size)\n",
    "    # sport = random.sample(sport, size)\n",
    "    # edu = random.sample(edu, size)\n",
    "    # tech = random.sample(tech, size)\n",
    "    with open(\"data/data2.txt\", \"a\") as f:\n",
    "        for line in story:\n",
    "            f.write(line)\n",
    "        for line in sport:\n",
    "            f.write(line)\n",
    "        for line in edu:\n",
    "            f.write(line)\n",
    "        for line in tech:\n",
    "            f.write(line)\n",
    "\n",
    "select_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145533\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sympy import apart\n",
    "\n",
    "def pre_process(path='data/data2.txt'):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    corpus = []\n",
    "    value = re.compile(r'^[\\u4e00-\\u9fa5]{2,}$')\n",
    "    try:\n",
    "        stopwords = open('data/stopwords.txt')\n",
    "        for word in stopwords:\n",
    "            word = word.strip()\n",
    "            texts.append(word)\n",
    "    except: \n",
    "        print('error')\n",
    "    \n",
    "    try:\n",
    "        with open(path) as f:    \n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                info = line.split('_!_')\n",
    "                label = info[1]\n",
    "                if label == '102':\n",
    "                    labels.append(0)\n",
    "                elif label == '103':\n",
    "                    labels.append(1)\n",
    "                elif label == '108':\n",
    "                    labels.append(2)\n",
    "                elif label == '109':\n",
    "                    labels.append(3)\n",
    "                data1 = jieba.cut(info[3])\n",
    "                data2 = info[4][:-1].split(',')\n",
    "                data_adj = ''\n",
    "                for item in data1:\n",
    "                    if item not in texts:\n",
    "                        if value.match(item):\n",
    "                            data_adj += item+' '\n",
    "                for item in data2:\n",
    "                    if item not in texts:\n",
    "                        if value.match(item):\n",
    "                            data_adj += item+' '\n",
    "                corpus.append(data_adj[:-1])\n",
    "                line = f.readline()\n",
    "    except:\n",
    "        print('error')\n",
    "    \n",
    "    with open('data/data3.txt', \"a\") as f:\n",
    "        for line in corpus:\n",
    "            f.write(line + '\\n')\n",
    "        print(len(corpus))\n",
    "\n",
    "pre_process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "414894a69326f3f6f0d879a0c415983bdd565510f9119cc362e056dc12c18af4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
