{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "from os import listdir\n",
    "import jieba\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    " \n",
    "all_file=listdir('***') #获取文件夹中所有文件名#数据集地址\n",
    "outputDir=\"***\" #结果输出地址\n",
    "labels=[] #用以存储名称\n",
    "corpus=[] #空语料库\n",
    "size=200#测试集容量\n",
    " \n",
    "def buildSW():\n",
    "    '''停用词的过滤'''\n",
    "    typetxt=open('***') #停用词文档地址\n",
    "    texts=['\\u3000','\\n',' '] #爬取的文本中未处理的特殊字符\n",
    "    '''停用词库的建立'''\n",
    "    for word in typetxt:\n",
    "        word=word.strip()\n",
    "        texts.append(word)\n",
    "    return texts\n",
    " \n",
    "def buildWB(texts):\n",
    "    '''语料库的建立'''\n",
    "    for i in range(0,len(all_file)):\n",
    "        filename=all_file[i]\n",
    "        filelabel=filename.split('.')[0]\n",
    "        labels.append(filelabel) #名称列表\n",
    "        file_add='***'+ filename #数据集地址\n",
    "        doc=open(file_add,encoding='utf-8').read()\n",
    "        data=jieba.cut(doc) #文本分词\n",
    "        data_adj=''\n",
    "        delete_word=[]\n",
    "        for item in data:\n",
    "            if item not in texts: #停用词过滤\n",
    "                # value=re.compile(r'^[0-9]+$')#去除数字\n",
    "                value = re.compile(r'^[\\u4e00-\\u9fa5]{2,}$')#只匹配中文2字词以上\n",
    "                if value.match(item):\n",
    "                    data_adj+=item+' '\n",
    "            else:\n",
    "                delete_word.append(item)\n",
    "        corpus.append(data_adj) #语料库建立完成\n",
    "    # print(corpus)\n",
    "    return corpus\n",
    " \n",
    "def countIdf(corpus):\n",
    "    vectorizer=CountVectorizer()#该类会将文本中的词语转换为词频矩阵，矩阵元素a[i][j] 表示j词在i类文本下的词频\n",
    "    transformer=TfidfTransformer()#该类会统计每个词语的tf-idf权值\n",
    "    tfidf=transformer.fit_transform(vectorizer.fit_transform(corpus))#第一个fit_transform是计算tf-idf，第二个fit_transform是将文本转为词频矩阵\n",
    "    weight=tfidf.toarray()#将tf-idf矩阵抽取出来，元素a[i][j]表示j词在i类文本中的tf-idf权重\n",
    "    # word=vectorizer.get_feature_names()#获取词袋模型中的所有词\n",
    "    # for j in range(len(word)):\n",
    "    #     if weight[1][j]!=0:\n",
    "    #         print(word[j], weight[1][j])\n",
    " \n",
    "    return weight\n",
    " \n",
    "def Kmeans(weight,clusters,correct):\n",
    "    mykms=KMeans(n_clusters=clusters)\n",
    "    y=mykms.fit_predict(weight)\n",
    "    result=[]\n",
    " \n",
    "    for i in range(0,clusters):\n",
    "        label_i=[]\n",
    "        gp=0\n",
    "        jy=0\n",
    "        xz=0\n",
    "        ty=0\n",
    "        for j in range(0,len(y)):\n",
    "            if y[j]==i:\n",
    "                label_i.append(labels[j])\n",
    "                type=labels[j][0:2]\n",
    "                if(type=='gp'):\n",
    "                    gp+=1\n",
    "                elif(type=='jy'):\n",
    "                    jy+=1\n",
    "                elif(type=='xz'):\n",
    "                    xz+=1\n",
    "                elif(type=='ty'):\n",
    "                    ty+=1\n",
    "        max=jy\n",
    "        type='教育'\n",
    "        if(gp>jy):\n",
    "            max=gp\n",
    "            type='股票'\n",
    "        if(max<xz):\n",
    "            max=xz\n",
    "            type='星座'\n",
    "        if(max<ty):\n",
    "            max=ty\n",
    "            type='体育'\n",
    "        correct[0]+=max\n",
    "        result.append('类别'+'('+type+')'+':'+str(label_i))\n",
    "    return result\n",
    " \n",
    "def output(result,outputDir,clusters):\n",
    "    outputFile='out'\n",
    "    type='.txt'\n",
    "    count=0\n",
    "    while(os.path.exists(outputDir+outputFile+type)):\n",
    "        count+=1\n",
    "        outputFile='out'+str(count)\n",
    "    doc = open(outputDir+outputFile+type, 'w')\n",
    "    for i in range(0,clusters):\n",
    "        print(result[i], file=doc)\n",
    "    print('本次分类总样本数目为:'+str(size)+' 其中正确分类数目为:'+str(correct[0])+' 正确率为：'+str(correct[0]/size), file=doc)\n",
    "    doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def getdata(path='data/toutiao_cat_data.txt'):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=buildSW()\n",
    "corpus=buildWB(texts)\n",
    "weight=countIdf(corpus)\n",
    "clusters=4\n",
    "correct=[0]#正确量\n",
    "result=Kmeans(weight,clusters,correct)\n",
    "output(result,outputDir,clusters)\n",
    "print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "414894a69326f3f6f0d879a0c415983bdd565510f9119cc362e056dc12c18af4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
